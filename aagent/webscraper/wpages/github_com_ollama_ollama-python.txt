


























































GitHub - ollama/ollama-python: Ollama Python library

















































Skip to content













Toggle navigation










          Sign in
        


 













        Product
        












Actions
        Automate any workflow
      







Packages
        Host and manage packages
      







Security
        Find and fix vulnerabilities
      







Codespaces
        Instant dev environments
      







Copilot
        Write better code with AI
      







Code review
        Manage code changes
      







Issues
        Plan and track work
      







Discussions
        Collaborate outside of code
      




Explore



      All features

    



      Documentation

    





      GitHub Skills

    





      Blog

    









        Solutions
        





For



      Enterprise

    



      Teams

    



      Startups

    



      Education

    






By Solution



      CI/CD & Automation

    



      DevOps

    



      DevSecOps

    






Resources



      Learning Pathways

    





      White papers, Ebooks, Webinars

    





      Customer Stories

    



      Partners

    









        Open Source
        









GitHub Sponsors
        Fund open source developers
      








The ReadME Project
        GitHub community articles
      




Repositories



      Topics

    



      Trending

    



      Collections

    






Pricing












Search or jump to...







Search code, repositories, users, issues, pull requests...

 




        Search
      













Clear
 















































 




              Search syntax tips
 














        Provide feedback
      









 
We read every piece of feedback, and take your input very seriously.


Include my email address so I can be contacted


     Cancel

    Submit feedback










        Saved searches
      
Use saved searches to filter your results more quickly









 





Name






Query



            To see all available qualifiers, see our documentation.
          
 





     Cancel

    Create saved search








              Sign in
            


              Sign up
            









You signed in with another tab or window. Reload to refresh your session.
You signed out in another tab or window. Reload to refresh your session.
You switched accounts on another tab or window. Reload to refresh your session.
 


Dismiss alert



















        ollama
 
/

ollama-python

Public





 

Notifications



 

Fork
    66




 


          Star
 1.1k
  












        Ollama Python library
      





ollama.ai


License





     MIT license
    






1.1k
          stars
 



66
          forks
 



Branches
 



Tags
 



Activity
 



 


          Star

  





 

Notifications














Code







Issues
17






Pull requests
9






Actions







Security







Insights



 

 


Additional navigation options


 









          Code










          Issues










          Pull requests










          Actions










          Security










          Insights





 





ollama/ollama-python







This commit does not belong to any branch on this repository, and may belong to a fork outside of the repository.

























    mainBranchesTagsGo to fileCodeFolders and filesNameNameLast commit messageLast commit dateLatest commit History83 Commits.github/workflows.github/workflows  examplesexamples  ollamaollama  teststests  .gitignore.gitignore  LICENSELICENSE  README.mdREADME.md  poetry.lockpoetry.lock  pyproject.tomlpyproject.toml  requirements.txtrequirements.txt  View all filesRepository files navigationREADMEMIT licenseOllama Python Library
The Ollama Python library provides the easiest way to integrate Python 3.8+ projects with Ollama.
Install
pip install ollama
Usage
import ollama
response = ollama.chat(model='llama2', messages=[
  {
    'role': 'user',
    'content': 'Why is the sky blue?',
  },
])
print(response['message']['content'])
Streaming responses
Response streaming can be enabled by setting stream=True, modifying function calls to return a Python generator where each part is an object in the stream.
import ollama

stream = ollama.chat(
    model='llama2',
    messages=[{'role': 'user', 'content': 'Why is the sky blue?'}],
    stream=True,
)

for chunk in stream:
  print(chunk['message']['content'], end='', flush=True)
API
The Ollama Python library's API is designed around the Ollama REST API
Chat
ollama.chat(model='llama2', messages=[{'role': 'user', 'content': 'Why is the sky blue?'}])
Generate
ollama.generate(model='llama2', prompt='Why is the sky blue?')
List
ollama.list()
Show
ollama.show('llama2')
Create
modelfile='''
FROM llama2
SYSTEM You are mario from super mario bros.
'''

ollama.create(model='example', modelfile=modelfile)
Copy
ollama.copy('llama2', 'user/llama2')
Delete
ollama.delete('llama2')
Pull
ollama.pull('llama2')
Push
ollama.push('user/llama2')
Embeddings
ollama.embeddings(model='llama2', prompt='They sky is blue because of rayleigh scattering')
Custom client
A custom client can be created with the following fields:

host: The Ollama host to connect to
timeout: The timeout for requests

from ollama import Client
client = Client(host='http://localhost:11434')
response = client.chat(model='llama2', messages=[
  {
    'role': 'user',
    'content': 'Why is the sky blue?',
  },
])
Async client
import asyncio
from ollama import AsyncClient

async def chat():
  message = {'role': 'user', 'content': 'Why is the sky blue?'}
  response = await AsyncClient().chat(model='llama2', messages=[message])

asyncio.run(chat())
Setting stream=True modifies functions to return a Python asynchronous generator:
import asyncio
from ollama import AsyncClient

async def chat():
  message = {'role': 'user', 'content': 'Why is the sky blue?'}
  async for part in await AsyncClient().chat(model='llama2', messages=[message], stream=True):
    print(part['message']['content'], end='', flush=True)

asyncio.run(chat())
Errors
Errors are raised if requests return an error status or if an error is detected while streaming.
model = 'does-not-yet-exist'

try:
  ollama.chat(model)
except ollama.ResponseError as e:
  print('Error:', e.error)
  if e.status_code == 404:
    ollama.pull(model)
   








About

        Ollama Python library
      





ollama.ai


Topics



  python


  ollama



Resources





        Readme
 
License





     MIT license
    








Activity
 





Custom properties
 
Stars





1.1k
      stars
 
Watchers





8
      watching
 
Forks





66
      forks
 


          Report repository
 







    Releases
      8







v0.1.7

          Latest
 
Mar 1, 2024

 

        + 7 releases







      Used by 365
 




























        + 357
      







    Contributors
      5
































Languages










Python
100.0%















Footer








        © 2024 GitHub, Inc.
      


Footer navigation


Terms


Privacy


Security


Status


Docs


Contact




      Manage cookies
    





      Do not share my personal information
    
















    You can’t perform that action at this time.
  












